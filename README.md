# laid-c-hdf5-mpi
OpenMPI/C implementation of LAID with matrices stored in hdf5 file

This branch implements the set cover algorithm as defined by the thesis of João Apolónia [Apolónia, João - Seleção de atributos de dados inconsistentes [Em linha]. Lisboa: [s.n.], 2018. 111 p.]

# Changes
The dataset used are generated using bitarrays, reducing disk and memory requirements for the datasets.

The disjoint matrix is generated by lines (A) and by columns (A<sup>T</sup>).
In the matriz A each line corresponds to an observation and each bit represents an attribute.
If A\[i\]\[j\] = 1, it means that the line i is covered by the attribute j.

In the matriz A<sup>T</sup> each line corresponds to an attribute and each bit represents a line.
If A<sup>T</sup>\[i\]\[j\] = 1, it means the atribute i covers the line j.

This allows us to get the covered lines array for an attribute much faster.

The main algorithm is:

* Read dataset attributes from hdf5 file
* Read dataset
* Sort dataset
* Remove duplicates
* Add jnsqs
* Write disjoint matrix by line (A)
* Write disjoint matrix by column (A<sup>T</sup>)
* Apply set covering algorithm
* Show solution

## Set covering algorithm
* Fill the attributes totals array from the generated disjoint matrix dataset (A)
* LOOP:
  * Select the best attribute (the one that cover most lines) and add it to the solution
  * Get covered lines array from A<sup>T</sup> for best attribute
  * Calculate the new attribute totals array.
  * goto LOOP

## Notes
* We  can check if the dataset file already has the necessary datasets generated and skip the creation step:
  * If the file already has a dataset with the name of the disjoing matrix dataset we assume it was built in a previous iteration and skip ahead for the cover algorithm.
* If we have to build the disjoint matrices:
  * Each node root will open the original dataset file and read the contents to memory. This allows us to save memory in each node, without sacrificing much performance, because we're not having to send data across nodes.
  * The node root(s) sort the dataset in memory, remove duplicates and add jnsqs bits if necessary.
  * Having access to the original dataset, each process generates a part of the final matrix and stores it in the hdf5 file.
 
* We can't access easily the attribute data using the line dataset because they are stored in words of size WORD_BITS (usually 64 bits).
* To read the values for a single attribute we'd need to read blocks of WORD_BITS attributes at once. This is wasteful and slow, because of the file seeking time, and because we then need to extract the info for one attribute from every word.
* To alleviate this we generate a new dataset where each line has the data for an attribute and the columns represent the observations. We can now easily get all the data for an attribute with a single read from the hdf5 file.

# TLDR:
## 1st phase
* ALL:
  * Reads dataset attributes from hdf5 file
* NODE ROOT:
  * Read dataset
  * Sort dataset
  * Remove duplicates
  * Add jnsqs
  * Share dataset with the remaining processes in node
 
* ALL:
  *  Write disjoint matrix datasets
    
## 2nd phase
* ALL:
  * Setup line covered array -> {0,0,0,...,0}
  * Setup attributes totals -> {0,0,0,...,0}
	 
* ROOT:
  * Reads the global attributes totals from the dataset file

* while there are still lines to blacklist:
  * ROOT
    * Selects the best attribute and marks it as selected
    * Sends attribute id to everyone else
  * ~ROOT:
    * Wait for attribute message
  * ALL:
   * Read column data for the selected attribute 
   * Update covered lines array
   * Update atributes totals
   * MPI_Reduce (MPI_SUM) attributes totals

* ROOT:
  * Show solution
